# Runtime image for serving GTA1 via provider's Ray Serve app
FROM vllm/vllm-openai:v0.10.1

WORKDIR /app

# Copy only the provider serving code and minimal deps footprint
COPY src/model_serving.py /app/src/model_serving.py

# Install runtime dependencies for provider path (keep base vLLM stack intact)
RUN pip install --no-cache-dir --upgrade pip && \
    pip install --no-cache-dir \
      "ray[serve]" \
      fastapi \
      pillow \
      requests

# Cache + service defaults
ENV HF_HOME=/runpod-volume/hf \
    HUGGINGFACE_HUB_CACHE=/runpod-volume/hf/hub \
    XDG_CACHE_HOME=/runpod-volume/.cache \
    TRITON_CACHE_DIR=/runpod-volume/.cache/triton \
    VLLM_CACHE_ROOT=/runpod-volume/.cache/vllm \
    MODEL_ID=Salesforce/GTA1-32B \
    PORT=3005 \
    NUM_REPLICAS=1 \
    TOKENIZERS_PARALLELISM=false \
    HF_HUB_ENABLE_HF_TRANSFER=1

# Expose providerâ€™s default port
EXPOSE 3005

# Run the provider app directly (no uvicorn; Ray Serve owns HTTP)
ENTRYPOINT ["python3", "-u", "/app/src/model_serving.py"]
CMD ["--model_path", "${MODEL_ID}", "--host", "0.0.0.0", "--port", "${PORT}", "--num_replicas", "${NUM_REPLICAS}"]

