# Runtime image for serving GTA1 via provider's Ray Serve app
FROM vllm/vllm-openai:v0.10.1

WORKDIR /app

# Install runtime dependencies early to maximize layer cache reuse
# (kept independent from app source changes)
RUN --mount=type=cache,target=/root/.cache/pip \
    pip install --upgrade pip && \
    pip install \
      "ray[serve]" \
      fastapi \
      blobfile \
      uvicorn[standard] \
      pillow \
      requests

# Copy provider serving code and entrypoint
COPY src/model_serving.py /app/src/model_serving.py
COPY docker/entrypoint.ray.sh /app/entrypoint.ray.sh
RUN chmod +x /app/entrypoint.ray.sh

# Cache + service defaults
ENV HF_HOME=/runpod-volume/hf \
    HUGGINGFACE_HUB_CACHE=/runpod-volume/hf/hub \
    XDG_CACHE_HOME=/runpod-volume/.cache \
    TRITON_CACHE_DIR=/runpod-volume/.cache/triton \
    VLLM_CACHE_ROOT=/runpod-volume/.cache/vllm \
    MAX_MODEL_LEN=24576 \
    GPU_MEMORY_UTILIZATION=0.92 \
    MODEL_ID=Adocados/GTA1-32B-vllm \
    TOKENIZER_ID=Adocados/GTA1-32B-vllm \
    TOKENIZER_MODE=slow \
    TRANSFORMERS_TRUST_REMOTE_CODE=1 \
    PORT=3005 \
    PORT_HEALTH=3006 \
    NUM_REPLICAS=1 \
    MODEL_ACTOR_CPUS=3.0 \
    APP_ACTOR_CPUS=0.5 \
    RAY_NUM_CPUS=4 \
    RAY_NUM_GPUS=1 \
    RAY_DISABLE_DOCKER_CPU_WARNING=1 \
    TOKENIZERS_PARALLELISM=false \
    HF_HUB_ENABLE_HF_TRANSFER=1

# Expose providerâ€™s default port
EXPOSE 3005 3006

# Entry runs health server and then Ray Serve app
ENTRYPOINT ["/app/entrypoint.ray.sh"]
