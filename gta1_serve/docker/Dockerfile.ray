# Runtime image for serving GTA1 via provider's Ray Serve app
FROM vllm/vllm-openai:v0.10.1

WORKDIR /app

# Copy only the provider serving code and minimal deps footprint
COPY src/model_serving.py /app/src/model_serving.py
COPY docker/entrypoint.ray.sh /app/entrypoint.ray.sh

# Install runtime dependencies for provider path (keep base vLLM stack intact)
RUN pip install --no-cache-dir --upgrade pip && \
    pip install --no-cache-dir \
      "ray[serve]" \
      fastapi \
      uvicorn[standard] \
      pillow \
      requests && \
    chmod +x /app/entrypoint.ray.sh

# Cache + service defaults
ENV HF_HOME=/runpod-volume/hf \
    HUGGINGFACE_HUB_CACHE=/runpod-volume/hf/hub \
    XDG_CACHE_HOME=/runpod-volume/.cache \
    TRITON_CACHE_DIR=/runpod-volume/.cache/triton \
    VLLM_CACHE_ROOT=/runpod-volume/.cache/vllm \
    MODEL_ID=Salesforce/GTA1-32B \
    PORT=3005 \
    PORT_HEALTH=3006 \
    NUM_REPLICAS=1 \
    TOKENIZERS_PARALLELISM=false \
    HF_HUB_ENABLE_HF_TRANSFER=1

# Expose providerâ€™s default port
EXPOSE 3005 3006

# Entry runs health server and then Ray Serve app
ENTRYPOINT ["/app/entrypoint.ray.sh"]
