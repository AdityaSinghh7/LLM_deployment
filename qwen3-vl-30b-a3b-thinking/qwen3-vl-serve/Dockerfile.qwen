# qwen3-vl-30b-a3b-thinking/qwen3-vl-serve/Dockerfile
# Pinned CUDA/PyTorch ABI via NGC to avoid flash-attn/kernel mismatch headaches.
FROM nvcr.io/nvidia/pytorch:24.08-py3

# ---- System + env ----------------------------------------------------------------
ENV DEBIAN_FRONTEND=noninteractive \
  PIP_NO_CACHE_DIR=1 \
  HF_HOME=/opt/cache/hf \
  XDG_CACHE_HOME=/opt/cache/xdg \
  HF_HUB_ENABLE_HF_TRANSFER=1 \
  OMP_NUM_THREADS=1 \
  PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True \
  NVIDIA_VISIBLE_DEVICES=all \
  NVIDIA_DRIVER_CAPABILITIES=compute,utility

RUN mkdir -p /opt/app /opt/cache/hf /opt/cache/xdg

# ---- Python deps (pin carefully) -------------------------------------------------
# vLLM 0.6.x has stable OpenAI-compatible server + VLM.
# Transformers >=4.44 needed for recent Qwen3-VL configs.
# uvicorn only for local debugging; server is vLLM http server.
RUN pip install --upgrade pip wheel setuptools \
  && pip install \
  "vllm==0.6.2" \
  "transformers==4.45.2" \
  "huggingface-hub==0.25.2" \
  "hf-transfer==0.1.6" \
  "fastapi==0.115.0" \
  "uvicorn==0.30.6" \
  && pip uninstall -y pynvml || true \
  && pip install nvidia-ml-py

# ---- Healthcheck helper -----------------------------------------------------------
COPY --chown=root:root <<'EOF' /opt/app/healthcheck.sh
#!/usr/bin/env bash
set -euo pipefail
curl -fsS "http://127.0.0.1:${PORT:-8000}/v1/models" >/dev/null
EOF
RUN chmod +x /opt/app/healthcheck.sh

# ---- Entrypoint wrapper -----------------------------------------------------------
COPY --chown=root:root <<'EOF' /opt/app/entrypoint.sh
#!/usr/bin/env bash
set -euo pipefail

# Resolve defaults without using the := assignment modifier
PORT="${PORT:-8000}"
VLLM_MODEL="${VLLM_MODEL:-Qwen/Qwen3-VL-30B-A3B-Thinking}"

# Sensible defaults; tune in your deployment manifests.
exec python -m vllm.entrypoints.openai.api_server \
  --host 0.0.0.0 \
  --port "${PORT}" \
  --model "${VLLM_MODEL}" \
  --trust-remote-code \
  --tensor-parallel-size "${TP_SIZE:-1}" \
  ${ENABLE_EXPERT_PARALLEL:+--enable-expert-parallel} \
  --max-model-len "${MAX_MODEL_LEN:-128000}" \
  --gpu-memory-utilization "${GPU_MEM_UTIL:-0.90}" \
  --async-scheduling \
  --limit-mm-per-prompt.video "${LIMIT_MM_VIDEO:-0}" \
  --mm-encoder-tp-mode "${MM_ENCODER_TP_MODE:-data}"
EOF
RUN chmod +x /opt/app/entrypoint.sh

EXPOSE 8000
HEALTHCHECK --interval=20s --timeout=5s --start-period=60s --retries=6 CMD /opt/app/healthcheck.sh

WORKDIR /opt/app
ENTRYPOINT ["/opt/app/entrypoint.sh"]
