# qwen3-vl-30b-a3b-thinking/qwen3-vl-serve/Dockerfile.qwen
# Pinned CUDA/PyTorch ABI via NGC to avoid flash-attn/kernel mismatch headaches.
FROM nvcr.io/nvidia/pytorch:24.08-py3

# ---- System + env ----------------------------------------------------------------
ENV DEBIAN_FRONTEND=noninteractive \
  PIP_NO_CACHE_DIR=1 \
  HF_HOME=/opt/cache/hf \
  XDG_CACHE_HOME=/opt/cache/xdg \
  HF_HUB_ENABLE_HF_TRANSFER=1 \
  OMP_NUM_THREADS=1 \
  PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True \
  NVIDIA_VISIBLE_DEVICES=all \
  NVIDIA_DRIVER_CAPABILITIES=compute,utility

# Tools needed for healthchecks etc.
RUN apt-get update && apt-get install -y --no-install-recommends \
  curl ca-certificates \
  && rm -rf /var/lib/apt/lists/*

RUN mkdir -p /opt/app /opt/cache/hf /opt/cache/xdg

# ---- Python deps (pin carefully) -------------------------------------------------
# vLLM 0.6.x has a stable OpenAI-compatible server + VLM.
# Transformers >=4.44 needed for recent Qwen3-VL configs.
RUN pip install --upgrade pip wheel setuptools \
  && pip install \
  "vllm==0.6.2" \
  "transformers==4.45.2" \
  "huggingface-hub==0.25.2" \
  "hf-transfer==0.1.6" \
  "fastapi==0.115.0" \
  "uvicorn==0.30.6" \
  && pip uninstall -y pynvml || true \
  && pip install nvidia-ml-py

# ---- Healthcheck helper -----------------------------------------------------------
# (kept inline; small and static)
COPY --chown=root:root <<'EOF' /opt/app/healthcheck.sh
#!/usr/bin/env bash
set -euo pipefail
curl -fsS "http://127.0.0.1:${PORT:-8000}/v1/models" >/dev/null
EOF
RUN chmod +x /opt/app/healthcheck.sh

# ---- Entrypoint (use your flag auto-detect script) --------------------------------
# IMPORTANT: this copies the entrypoint.sh that already lives in this folder.
COPY --chown=root:root entrypoint.sh /opt/app/entrypoint.sh
RUN chmod +x /opt/app/entrypoint.sh

EXPOSE 8000
HEALTHCHECK --interval=20s --timeout=5s --start-period=60s --retries=6 CMD /opt/app/healthcheck.sh

WORKDIR /opt/app
ENTRYPOINT ["/opt/app/entrypoint.sh"]
